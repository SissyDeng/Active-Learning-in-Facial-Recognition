{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is for the SubSampler Class, which we will use to train the model using only a fraction of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data.sampler as samplers\n",
    "\n",
    "class SubsetSampler(samplers.Sampler):\n",
    "\n",
    "    def __init__(self, indices):\n",
    "        self.indices = indices\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in range(len(self.indices)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage:\n",
    "\n",
    "- SubsetSampler takes the indices of the records of interest as an input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the implementation for the ActiveStrategy Class.\n",
    "An instance of that class is the controller for all Active Learning-related manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveStrategy(object):\n",
    "\n",
    "    def __init__(self, neuralNet, nsteps, clear=True, verbose=True):\n",
    "        self.clear = clear\n",
    "        self.verbose = verbose\n",
    "        self.net = neuralNet\n",
    "        self.nsteps = nsteps\n",
    "        self.train_length = len(trainset)\n",
    "        self.test_length = len(testset)\n",
    "        self.train_lbls = {}\n",
    "        self.test_lbls = {}\n",
    "        self.train_ind = {}\n",
    "        self.test_ind = {}\n",
    "        self.init_stats()\n",
    "        self.train_filter = [ i for i in range(self.train_length)]\n",
    "        self.test_filter = [ i for i in range(self.test_length)]\n",
    "        self.train_sampler = SubsetSampler(self.train_filter)\n",
    "        self.test_sampler = SubsetSampler(self.test_filter)\n",
    "        self.statsloader = torch.utils.data.DataLoader(trainset,\n",
    "                                                       shuffle=False,\n",
    "                                                       batch_size=1,\n",
    "                                                       num_workers=1)\n",
    "        self.trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                                       shuffle=False,\n",
    "                                                       batch_size=1,\n",
    "                                                       num_workers=1)\n",
    "        self.testloader  = torch.utils.data.DataLoader(testset,\n",
    "                                                       shuffle=False,\n",
    "                                                       batch_size=1,\n",
    "                                                       num_workers=1)\n",
    "        self.load()\n",
    "        self.experiments = []\n",
    "\n",
    "    def init_stats(self):\n",
    "        self.stats = {}\n",
    "        empty_dict = {}\n",
    "        for i in range(self.nsteps + 1):\n",
    "            empty_dict[i] = 0\n",
    "        for cl in classes:\n",
    "            self.stats[cl] = empty_dict.copy()\n",
    "            self.train_ind[cl] = []\n",
    "            self.test_ind[cl] = []\n",
    "\n",
    "    def update_stats(self, cl, sl):\n",
    "        self.stats[cl][0]    += 1\n",
    "        self.stats[cl][sl+1] += 1\n",
    "\n",
    "    def load(self):\n",
    "        for i, data in enumerate(self.statsloader, 0):\n",
    "            inputs, labels = data\n",
    "            sl = int(float(i) / self.train_length * self.nsteps)\n",
    "            self.update_stats(classes[labels[0]], sl)\n",
    "        for i, data in enumerate(self.trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            self.train_lbls[i] = classes[labels[0]]\n",
    "            #self.train_ind[classes[labels[0]]].append(i)\n",
    "        for i, data in enumerate(self.testloader, 0):\n",
    "            inputs, labels = data\n",
    "            self.test_lbls[i] = classes[labels[0]]\n",
    "            #self.test_ind[classes[labels[0]]].append(i)\n",
    "\n",
    "    def init_loaders(self):\n",
    "        self.trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                                       shuffle=False,\n",
    "                                                       batch_size=1,\n",
    "                                                       num_workers=1,\n",
    "                                                       sampler=self.train_sampler)\n",
    "        #self.test_sampler = SubsetSampler(self.test_filter[0:100])\n",
    "        self.testloader = torch.utils.data.DataLoader(testset,\n",
    "                                                      shuffle=False,\n",
    "                                                      batch_size=1,\n",
    "                                                      num_workers=1,\n",
    "                                                      sampler=self.test_sampler)\n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "\n",
    "    def incremental_supervised(self):\n",
    "        np.random.shuffle(self.train_filter)\n",
    "        \n",
    "    def load_strategy(self, selected):\n",
    "        self.train_filter = selected\n",
    "        \n",
    "    def train(self):\n",
    "\n",
    "        if self.clear:\n",
    "            self.initialize_weights()\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(self.net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "        for epoch in range(2):\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(self.trainloader, 0):\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = self.net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                if self.verbose:\n",
    "                    running_loss += loss.item()\n",
    "                    if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                        print('[%d, %5d] loss: %.3f' %\n",
    "                            (epoch + 1, i + 1, running_loss / 2000))\n",
    "                        running_loss = 0.0\n",
    "\n",
    "        print('Finished Training')\n",
    "        \n",
    "    def test(self):\n",
    "\n",
    "        dataiter = iter(self.testloader)\n",
    "        images, labels = dataiter.next()\n",
    "\n",
    "        print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(1)))\n",
    "\n",
    "        outputs = self.net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(1)))\n",
    "\n",
    "        soft = torch.nn.Softmax(dim=0)\n",
    "\n",
    "        ##### Stats below ######\n",
    "\n",
    "        ground_truth = []\n",
    "        predictions  = []\n",
    "        probabilities = []\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in self.testloader:\n",
    "                images, labels = data\n",
    "                outputs = self.net(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                ground_truth.append(labels.item())\n",
    "                predictions.append(predicted.item())\n",
    "                probabilities.append(soft(outputs[0]))\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Accuracy of the network on the {0} test images: {1}%'\n",
    "                .format(self.test_length, 100 * correct / total))\n",
    "\n",
    "        class_correct = {}\n",
    "        class_total   = {}\n",
    "        class_pred    = {}\n",
    "        for cl in classes:\n",
    "            class_correct[cl] = 0\n",
    "            class_total[cl]   = 0\n",
    "            class_pred[cl]    = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in self.testloader:\n",
    "                images, labels = data\n",
    "                outputs = self.net(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                c = (predicted == labels).squeeze()\n",
    "                # hard-coded for now  (.item() to avoid warning - not true if batch_size > 1) (J.Prendki)\n",
    "                label = labels.item()\n",
    "                class_correct[classes[label]] += c.item()\n",
    "                class_total[classes[label]] += 1\n",
    "                class_pred[classes[predicted.item()]] += 1\n",
    "\n",
    "        if self.verbose:\n",
    "            for cl in classes:\n",
    "                precision = class_correct[cl] / class_total[cl]\n",
    "                Fscore = -99.0\n",
    "                if class_pred[cl] > 0:\n",
    "                    recall = class_correct[cl] / class_pred[cl]\n",
    "                    Fscore    = 2.0 * precision * recall / (precision + recall)\n",
    "                print('%5s : \\t Accuracy: %2d %% \\t F-Score %.2f' % (\n",
    "                        cl,\n",
    "                        100.0 * precision,\n",
    "                        Fscore))\n",
    "\n",
    "        return correct / total, ground_truth, predictions, probabilities\n",
    "\n",
    "    def infer(self, sample):\n",
    "        sampler = SubsetSampler(sample)\n",
    "        dataloader = torch.utils.data.DataLoader(trainset,\n",
    "                                                 shuffle=False,\n",
    "                                                 batch_size=1,\n",
    "                                                 num_workers=4,\n",
    "                                                 sampler=sampler)\n",
    "        soft = torch.nn.Softmax(dim=0)\n",
    "        results = []\n",
    "        with torch.no_grad():\n",
    "            for r, rec in enumerate(dataloader):\n",
    "                # Evan (2019/11/15): I added this line, convert the indices from 0~47999 to 0~49999\n",
    "                r = sample[r] \n",
    "\n",
    "                images, labels = rec\n",
    "                outputs = self.net(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                ground_truth = labels.item()\n",
    "                prediction   = predicted.item()\n",
    "                probability  = soft(outputs[0])\n",
    "                classwiseprobs = probability.numpy()\n",
    "                results.append([r, classes[ground_truth], classes[prediction], probability[prediction],classwiseprobs])\n",
    "        return results\n",
    "    \n",
    "    def run_one(self, selected):\n",
    "        self.load_strategy(selected)\n",
    "        results = []\n",
    "        if self.clear:\n",
    "            self.initialize_weights()\n",
    "            print(\"Network's weights reinitialized\")\n",
    "        print(\"Training for {0} records:\".format(len(selected)))\n",
    "        self.train_sampler = SubsetSampler(self.train_filter)\n",
    "        self.trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                                        shuffle=False,\n",
    "                                                        batch_size=1,\n",
    "                                                        num_workers=1,\n",
    "                                                        sampler=self.train_sampler)\n",
    "        self.train()\n",
    "        res, truth, outs, probs = self.test()\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def run_experiment(self, nsteps, maximum):\n",
    "        results = []\n",
    "        for n in range(1, nsteps+1):\n",
    "            if self.clear:\n",
    "                self.initialize_weights()\n",
    "                print(\"Network's weights reinitialized\")\n",
    "            nsamples = int(1.0 / nsteps * n * maximum)\n",
    "            print(\"Training for {0} samples:\".format(nsamples))\n",
    "            self.train_sampler = SubsetSampler(self.train_filter[:nsamples])\n",
    "            self.trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                                           shuffle=False,\n",
    "                                                           batch_size=1,\n",
    "                                                           num_workers=4,\n",
    "                                                           sampler=self.train_sampler)\n",
    "            self.train()\n",
    "            res, truth, outs, probs = self.test()\n",
    "            results.append(res)\n",
    "\n",
    "        return results\n",
    "    \n",
    "    def run_ConfidenceAL(self, qStrategy, nsteps, maximum):\n",
    "    \n",
    "        results = []\n",
    "    \n",
    "        unlabeled = [i for i in range(len(trainset))]\n",
    "        labeled   = []\n",
    "\n",
    "        to_be_labeled = random.sample(unlabeled, int(nps))\n",
    "        unlabeled = list(set(unlabeled)-set(to_be_labeled))\n",
    "        myres = self.run_one(to_be_labeled)\n",
    "        results.append(myres)\n",
    "    \n",
    "        for n in range(1, nsteps):\n",
    "            myResults = self.infer(unlabeled)\n",
    "            to_be_labeled.extend( qStrategy(myResults, int(maximum/nsteps)) ) # updating function\n",
    "            unlabeled = list(set(unlabeled)-set(to_be_labeled))\n",
    "            myres = self.run_one(to_be_labeled)\n",
    "            results.append(myres)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_StreamingAL(self, qStrategy, nsteps, maximum):\n",
    "    \n",
    "        results = []\n",
    "        stepSizes = []\n",
    "    \n",
    "        unlabeled = [i for i in range(len(trainset))]\n",
    "        labeled   = []\n",
    "\n",
    "        to_be_labeled = random.sample(unlabeled, int(nps))\n",
    "        unlabeled = list(set(unlabeled)-set(to_be_labeled))\n",
    "        myres = self.run_one(to_be_labeled)\n",
    "        results.append(myres)\n",
    "        stepSizes.append(len(to_be_labeled))\n",
    "    \n",
    "        for n in range(1, nsteps):\n",
    "            myResults = self.infer(unlabeled)\n",
    "            to_be_labeled.extend( qStrategy(myResults) ) # updating function\n",
    "            if (len(to_be_labeled) > maximum):\n",
    "                break\n",
    "            unlabeled = list(set(unlabeled)-set(to_be_labeled))\n",
    "            myres = self.run_one(to_be_labeled)\n",
    "            results.append(myres)\n",
    "            stepSizes.append(len(to_be_labeled))\n",
    "        \n",
    "        return results, stepSizes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ALStrategy.infer(sample): run inferrences on all records in sample\n",
    "\n",
    "- ALStrategy.run_experiment(num_steps, maximum): design to experiment on random samples (incremental supervised learning)\n",
    "\n",
    "- ALStrategy.run_ConfidenceAL(qStrategy, num_steps, maximum): running a querying strategy of type confidence-level or, more generally, uncertainty (Pooling approach)\n",
    "\n",
    "- ALStrategy.run_StreamingAL(qStrategy, num_steps, maximum): running a querying strategy with a Streaming approach"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
