{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is the metaDataFrame class for capturing the metadata from the previous loops and recording them. There will be strategies that are based on particular scenarios, i.e. High-Variance (HV), Low-Value-Low-Variance (LV2), and consistent-negative-change (CNC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class metaDataFrame(object):\n",
    "    def __init__(self, inferIdx):\n",
    "        self.df = pd.DataFrame(index=inferIdx)\n",
    "        \n",
    "    # Function: Assign the ground truth\n",
    "    # This is only used for experimentation purposes, as the ground truth label is normally not available for training data.\n",
    "    def assignGroundTruth(self, groundTruth):\n",
    "        gt_idx = [row[0] for row in groundTruth]\n",
    "        gt_series = pd.Series([row[1] for row in groundTruth], index=gt_idx)\n",
    "        \n",
    "        self.df['ground_truth'] = gt_series\n",
    "        \n",
    "    # Function: Record the metadata from the AL loop\n",
    "    def appendMetadata(self, metaType, iterNum, inferResults):\n",
    "        class_name = 'class_' + str(iterNum)\n",
    "        meta_name = metaType + '_' + str(iterNum)\n",
    "        \n",
    "        infer_idx = [row[0] for row in inferResults]\n",
    "        infer_pred = pd.Series([row[2] for row in inferResults], index=infer_idx)\n",
    "        infer_prob = pd.Series([row[3] for row in inferResults], index=infer_idx)\n",
    "        \n",
    "        # Create df from the prediction and probability series\n",
    "        frame = { class_name: infer_pred, meta_name: infer_prob } \n",
    "        infer_df = pd.DataFrame(frame)\n",
    "        \n",
    "        self.df = self.df.merge(infer_df, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    # Function: Calculate the mean of the metadata\n",
    "    def calculateAverage(self, metaType, k):\n",
    "        loops = [col for col in self.df.columns.tolist() if metaType in col]\n",
    "        \n",
    "        # Check that the meta dataframe has at least 2 metadata cols and >= k\n",
    "        if len(loops) >= 2 and len(loops) >= k:\n",
    "            metadata_avg = []\n",
    "            \n",
    "            mask = [col for col in self.df.columns.tolist() if metaType in col][-k:]\n",
    "            metadata_avg = self.df[mask].apply(lambda x: x.mean(), axis=1)\n",
    "        else:\n",
    "            print('The meta dataframe has no iterations record to compute the average.')\n",
    "        \n",
    "        return metadata_avg\n",
    "    \n",
    "    # Function: Calculate the variance in the metadata\n",
    "    def calculateVariance(self, metaType, k):\n",
    "        loops = [col for col in self.df.columns.tolist() if metaType in col]\n",
    "        \n",
    "        # Check that the meta dataframe has at least 2 metadata cols and >= k\n",
    "        if len(loops) >= 2 and len(loops) >= k:\n",
    "            metadata_var = []\n",
    "            \n",
    "            mask = [col for col in self.df.columns.tolist() if metaType in col][-k:]\n",
    "            metadata_var = self.df[mask].apply(lambda x: x.var(), axis=1)\n",
    "        else:\n",
    "            print('The meta dataframe has no iterations record to compute the variance.')\n",
    "        \n",
    "        return metadata_var\n",
    "    \n",
    "    # Function: Calculate the net difference in the metadata of k loops\n",
    "    def calculateNetDiff(self, metaType, k):\n",
    "        loops = [col for col in self.df.columns.tolist() if metaType in col]\n",
    "        \n",
    "        # Check that the dataframe has at least 2 metadata cols and >= k\n",
    "        if len(loops) >= 2 and len(loops) >= k:\n",
    "            metadata_diff = []\n",
    "            \n",
    "            mask = [col for col in self.df.columns.tolist() if metaType in col][-k:]            \n",
    "            metadata_diff = self.df[mask].diff(axis=1)\n",
    "            net_diff = metadata_diff[metadata_diff.columns[-k+1:]].values.tolist()\n",
    "            \n",
    "#             metadata_diff = self.df[mask].apply(lambda x: [x[i+1] - x[i] for i in range(len(x)-1)])\n",
    "#             metadata_netdiff = metadata_diff.sum()\n",
    "        else:\n",
    "            print('The meta dataframe has no iterations record to compute the net difference.')\n",
    "        \n",
    "        return  net_diff\n",
    "    \n",
    "    # Function: Create a list of predicted classes across loops\n",
    "    def createPredClassList(self, k):\n",
    "        loops = [col for col in self.df.columns.tolist() if 'class' in col]\n",
    "        \n",
    "        # Check that the meta dataframe has at least 2 metadata cols and >= k\n",
    "        if len(loops) >= 2 and len(loops) >= k:\n",
    "            mask = [col for col in self.df.columns.tolist() if 'class' in col][-k:]\n",
    "            pred_classes = self.df[mask].values.tolist()\n",
    "        \n",
    "        return pred_classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to identify the data points that fall into specific multi-gen scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Output the indexes of the data that pertain to the High-Variance (HV) scenario according to the set params.\n",
    "def queryHV(varList, varThresh):\n",
    "    hv_bool = varList >= varThresh\n",
    "    hv_index = hv_bool[hv_bool].index\n",
    "    \n",
    "    return pd.Series(hv_index)\n",
    "\n",
    "\n",
    "# Function: Output the indexes of the data that pertain to the Low-Value-Low-Variance (LV2) scenario according to the set params.\n",
    "def queryLV2(avgList, varList, avgThresh, varThresh):\n",
    "    lv_bool = (avgList <= avgThresh) & (varList <= varThresh)\n",
    "    lv2_index = lv_bool[lv_bool].index\n",
    "    \n",
    "    return pd.Series(lv2_index)\n",
    "\n",
    "\n",
    "# Function: Output the indexes of the data that pertain to the Consistent-Negative-Change (CNC) scenario according to the set params.\n",
    "def queryCNC(net_diff, negThresh, changeThresh):\n",
    "    # Logic here to differentiate negative differences\n",
    "    # 1. Instead of an actual count, the threshold may have to be a ratio? > 0.6? # of diff / k\n",
    "    # 2. Weight average of the differences\n",
    "    # Maybe make this an if statement with choices?\n",
    "    k = len(net_diff[0])\n",
    "    change_list = np.array(net_diff) <= negThresh\n",
    "    change_count = np.sum(change_list, 1)\n",
    "    coc_bool = change_count >= changeThresh\n",
    "    coc_index = np.where(coc_bool)\n",
    "    \n",
    "    return pd.Series(coc_index)\n",
    "    \n",
    "\n",
    "# Function: Output the indexes of the data that pertain to the Change-of-Class (CoC) scenario according to the set params.\n",
    "def queryCoC(predClasses, changeThresh):\n",
    "    change_list = np.array([[(row[i] != row[i+1]) for i in range(len(row)-1)] for row in predClasses])\n",
    "    change_count = np.sum(change_list, 1)\n",
    "    coc_bool = change_count >= changeThresh\n",
    "    coc_index = list(*np.where(coc_bool))\n",
    "    \n",
    "    return pd.Series(coc_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low hanging fruit = we have experiment results, both Fashion-MNIST and facial recog.\n",
    "# We don't even know whether these 4 scenarios > baseline.\n",
    "\n",
    "# HV, LV2, CNC, CoC: high hanging fruit, nice to have.\n",
    "# Build a greedy algorithm, that evaluates every one of these 4 scenarios at every loop.-> how much of an improvement in model performance.\n",
    "# 1. Take the most effective approach at every loop after the evaluation\n",
    "# 2. A weighted average of all the approaches, where it learns the weights over time.\n",
    "# Computationally heavy. <- not sure if it can handle the load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with mock up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_1</th>\n",
       "      <th>confidence_1</th>\n",
       "      <th>class_2</th>\n",
       "      <th>confidence_2</th>\n",
       "      <th>class_3</th>\n",
       "      <th>confidence_3</th>\n",
       "      <th>class_4</th>\n",
       "      <th>confidence_4</th>\n",
       "      <th>class_5</th>\n",
       "      <th>confidence_5</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>class2</td>\n",
       "      <td>0.55</td>\n",
       "      <td>class2</td>\n",
       "      <td>0.68</td>\n",
       "      <td>class2</td>\n",
       "      <td>0.81</td>\n",
       "      <td>class2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>class2</td>\n",
       "      <td>0.89</td>\n",
       "      <td>class2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>class2</td>\n",
       "      <td>0.92</td>\n",
       "      <td>class1</td>\n",
       "      <td>0.30</td>\n",
       "      <td>class2</td>\n",
       "      <td>0.68</td>\n",
       "      <td>class2</td>\n",
       "      <td>0.72</td>\n",
       "      <td>class1</td>\n",
       "      <td>0.37</td>\n",
       "      <td>class1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>class2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>class2</td>\n",
       "      <td>0.22</td>\n",
       "      <td>class1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>class2</td>\n",
       "      <td>0.19</td>\n",
       "      <td>class2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>class1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>class2</td>\n",
       "      <td>0.78</td>\n",
       "      <td>class2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>class2</td>\n",
       "      <td>0.67</td>\n",
       "      <td>class2</td>\n",
       "      <td>0.51</td>\n",
       "      <td>class2</td>\n",
       "      <td>0.52</td>\n",
       "      <td>class2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>class2</td>\n",
       "      <td>0.82</td>\n",
       "      <td>class1</td>\n",
       "      <td>0.68</td>\n",
       "      <td>class2</td>\n",
       "      <td>0.72</td>\n",
       "      <td>class1</td>\n",
       "      <td>0.88</td>\n",
       "      <td>class2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>class2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class_1  confidence_1 class_2  confidence_2 class_3  confidence_3 class_4  \\\n",
       "1  class2          0.55  class2          0.68  class2          0.81  class2   \n",
       "2  class2          0.92  class1          0.30  class2          0.68  class2   \n",
       "3  class2          0.12  class2          0.22  class1          0.15  class2   \n",
       "5  class2          0.78  class2          0.75  class2          0.67  class2   \n",
       "6  class2          0.82  class1          0.68  class2          0.72  class1   \n",
       "\n",
       "   confidence_4 class_5  confidence_5 ground_truth  \n",
       "1          0.90  class2          0.89       class2  \n",
       "2          0.72  class1          0.37       class1  \n",
       "3          0.19  class2          0.21       class1  \n",
       "5          0.51  class2          0.52       class2  \n",
       "6          0.88  class2          0.90       class2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample test data to test the different multi-gen scenario functions\n",
    "# Sample scenarios:\n",
    "# 1. Not informative\n",
    "# 2. HV\n",
    "# 3. LV2\n",
    "# 4. CNC\n",
    "# 5. CoC\n",
    "meta_df = metaDataFrame([1, 2, 3, 5, 6])\n",
    "\n",
    "meta_df.appendMetadata('confidence', 1, [[1, 'class2', 'class2', .55], [2, 'class2', 'class2', .92], [3, 'class2', 'class2', .12], [5, 'class2', 'class2', .78], [6, 'class2', 'class2', .82]])\n",
    "meta_df.appendMetadata('confidence', 2, [[1, 'class1', 'class2', .68], [2, 'class1', 'class1', .30], [3, 'class1', 'class2', .22], [5, 'class1', 'class2', .75], [6, 'class1', 'class1', .68]])\n",
    "meta_df.appendMetadata('confidence', 3, [[1, 'class1', 'class2', .81], [2, 'class1', 'class2', .68], [3, 'class1', 'class1', .15], [5, 'class1', 'class2', .67], [6, 'class1', 'class2', .72]])\n",
    "meta_df.appendMetadata('confidence', 4, [[1, 'class2', 'class2', .90], [2, 'class2', 'class2', .72], [3, 'class2', 'class2', .19], [5, 'class2', 'class2', .51], [6, 'class2', 'class1', .88]])\n",
    "meta_df.appendMetadata('confidence', 5, [[1, 'class2', 'class2', .89], [2, 'class2', 'class1', .37], [3, 'class2', 'class2', .21], [5, 'class2', 'class2', .52], [6, 'class2', 'class2', .90]])\n",
    "meta_df.assignGroundTruth([[1, 'class2'], [2, 'class1'], [3, 'class1'], [5, 'class2'], [6, 'class2']])\n",
    "\n",
    "meta_df.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.002433\n",
      "2    0.036700\n",
      "3    0.000933\n",
      "5    0.008033\n",
      "6    0.009733\n",
      "dtype: float64\n",
      "1    0.866667\n",
      "2    0.590000\n",
      "3    0.183333\n",
      "5    0.566667\n",
      "6    0.833333\n",
      "dtype: float64\n",
      "[['class2', 'class2', 'class2'], ['class2', 'class2', 'class1'], ['class1', 'class2', 'class2'], ['class2', 'class2', 'class2'], ['class2', 'class1', 'class2']]\n"
     ]
    }
   ],
   "source": [
    "# Test the different class helper functions\n",
    "var_list = meta_df.calculateVariance('confidence', 3)\n",
    "print(var_list)\n",
    "\n",
    "avg_list = meta_df.calculateAverage('confidence', 3)\n",
    "print(avg_list)\n",
    "\n",
    "pred_classes = meta_df.createPredClassList(3)\n",
    "print(pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    2\n",
      "2    5\n",
      "3    6\n",
      "dtype: int64\n",
      "0    3\n",
      "dtype: int64\n",
      "0    [1, 3]\n",
      "dtype: object\n",
      "0    1\n",
      "1    2\n",
      "2    4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# HV scenario test\n",
    "HV_test = queryHV(var_list, 0.001)\n",
    "print(HV_test)\n",
    "\n",
    "# LV2 scenario test\n",
    "LV2_test = queryLV2(avg_list, var_list, .4, 0.001)\n",
    "print(LV2_test)\n",
    "\n",
    "# CNC scenario test\n",
    "CNC_test = queryCNC(net_diff, -.01, 2)\n",
    "print(CNC_test)\n",
    "\n",
    "# CoC scenario test\n",
    "CoC_test = queryCoC(pred_classes, 1)\n",
    "print(CoC_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13, 0.13, 0.08999999999999997, -0.010000000000000009], [-0.6200000000000001, 0.38000000000000006, 0.039999999999999925, -0.35], [0.1, -0.07, 0.04000000000000001, 0.01999999999999999], [-0.030000000000000027, -0.07999999999999996, -0.16000000000000003, 0.010000000000000009], [-0.1399999999999999, 0.039999999999999925, 0.16000000000000003, 0.020000000000000018]]\n"
     ]
    }
   ],
   "source": [
    "print(meta_df.calculateNetDiff('confidence', 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [1, 3]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "net_diff = meta_df.calculateNetDiff('confidence', 5)\n",
    "print(queryCNC(net_diff, -.01, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
