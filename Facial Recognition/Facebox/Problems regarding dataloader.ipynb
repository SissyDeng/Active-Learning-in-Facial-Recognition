{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o WIDER_train.zip -d ./FaceBoxes.PyTorch/data/WIDER_FACE/images/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download WIDER FACE dataset(WIDER_train.zip), place the images under this directory:\n",
    "\n",
    "$FaceBoxes.PyTorch/data/WIDER_FACE/images\n",
    "\n",
    "2. Convert WIDER FACE annotations to VOC format or download the converted annotations(annotations.rar), place them under this directory:\n",
    "\n",
    "$FaceBoxes.PyTorch/data/WIDER_FACE/annotations\n",
    "\n",
    "3. Also place the img_list.txt under this directory:\n",
    "\n",
    "$FaceBoxes.PyTorch/data/WIDER_FACE/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def matrix_iou(a, b):\n",
    "    \"\"\"\n",
    "    return iou of a and b, numpy version for data augenmentation\n",
    "    \"\"\"\n",
    "    lt = np.maximum(a[:, np.newaxis, :2], b[:, :2])\n",
    "    rb = np.minimum(a[:, np.newaxis, 2:], b[:, 2:])\n",
    "\n",
    "    area_i = np.prod(rb - lt, axis=2) * (lt < rb).all(axis=2)\n",
    "    area_a = np.prod(a[:, 2:] - a[:, :2], axis=1)\n",
    "    area_b = np.prod(b[:, 2:] - b[:, :2], axis=1)\n",
    "    return area_i / (area_a[:, np.newaxis] + area_b - area_i)\n",
    "\n",
    "def matrix_iof(a, b):\n",
    "    \"\"\"\n",
    "    return iof of a and b, numpy version for data augenmentation\n",
    "    \"\"\"\n",
    "    lt = np.maximum(a[:, np.newaxis, :2], b[:, :2])\n",
    "    rb = np.minimum(a[:, np.newaxis, 2:], b[:, 2:])\n",
    "\n",
    "    area_i = np.prod(rb - lt, axis=2) * (lt < rb).all(axis=2)\n",
    "    area_a = np.prod(a[:, 2:] - a[:, :2], axis=1)\n",
    "    return area_i / np.maximum(area_a[:, np.newaxis], 1)\n",
    "\n",
    "\n",
    "def _crop(image, boxes, labels, img_dim):\n",
    "    height, width, _ = image.shape\n",
    "    pad_image_flag = True\n",
    "\n",
    "    for _ in range(250):\n",
    "        if random.uniform(0, 1) <= 0.2:\n",
    "            scale = 1\n",
    "        else:\n",
    "            scale = random.uniform(0.3, 1.)\n",
    "        short_side = min(width, height)\n",
    "        w = int(scale * short_side)\n",
    "        h = w\n",
    "\n",
    "        if width == w:\n",
    "            l = 0\n",
    "        else:\n",
    "            l = random.randrange(width - w)\n",
    "        if height == h:\n",
    "            t = 0\n",
    "        else:\n",
    "            t = random.randrange(height - h)\n",
    "        roi = np.array((l, t, l + w, t + h))\n",
    "\n",
    "        value = matrix_iof(boxes, roi[np.newaxis])\n",
    "        flag = (value >= 1)\n",
    "        if not flag.any():\n",
    "            continue\n",
    "\n",
    "        centers = (boxes[:, :2] + boxes[:, 2:]) / 2\n",
    "        mask_a = np.logical_and(roi[:2] < centers, centers < roi[2:]).all(axis=1)\n",
    "        boxes_t = boxes[mask_a].copy()\n",
    "        labels_t = labels[mask_a].copy()\n",
    "\n",
    "        if boxes_t.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        image_t = image[roi[1]:roi[3], roi[0]:roi[2]]\n",
    "\n",
    "        boxes_t[:, :2] = np.maximum(boxes_t[:, :2], roi[:2])\n",
    "        boxes_t[:, :2] -= roi[:2]\n",
    "        boxes_t[:, 2:] = np.minimum(boxes_t[:, 2:], roi[2:])\n",
    "        boxes_t[:, 2:] -= roi[:2]\n",
    "\n",
    "\t# make sure that the cropped image contains at least one face > 16 pixel at training image scale\n",
    "        b_w_t = (boxes_t[:, 2] - boxes_t[:, 0] + 1) / w * img_dim\n",
    "        b_h_t = (boxes_t[:, 3] - boxes_t[:, 1] + 1) / h * img_dim\n",
    "        mask_b = np.minimum(b_w_t, b_h_t) > 16.0\n",
    "        boxes_t = boxes_t[mask_b]\n",
    "        labels_t = labels_t[mask_b]\n",
    "\n",
    "        if boxes_t.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        pad_image_flag = False\n",
    "\n",
    "        return image_t, boxes_t, labels_t, pad_image_flag\n",
    "    return image, boxes, labels, pad_image_flag\n",
    "\n",
    "\n",
    "def _distort(image):\n",
    "\n",
    "    def _convert(image, alpha=1, beta=0):\n",
    "        tmp = image.astype(float) * alpha + beta\n",
    "        tmp[tmp < 0] = 0\n",
    "        tmp[tmp > 255] = 255\n",
    "        image[:] = tmp\n",
    "\n",
    "    image = image.copy()\n",
    "\n",
    "    if random.randrange(2):\n",
    "\n",
    "        #brightness distortion\n",
    "        if random.randrange(2):\n",
    "            _convert(image, beta=random.uniform(-32, 32))\n",
    "\n",
    "        #contrast distortion\n",
    "        if random.randrange(2):\n",
    "            _convert(image, alpha=random.uniform(0.5, 1.5))\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        #saturation distortion\n",
    "        if random.randrange(2):\n",
    "            _convert(image[:, :, 1], alpha=random.uniform(0.5, 1.5))\n",
    "\n",
    "        #hue distortion\n",
    "        if random.randrange(2):\n",
    "            tmp = image[:, :, 0].astype(int) + random.randint(-18, 18)\n",
    "            tmp %= 180\n",
    "            image[:, :, 0] = tmp\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    else:\n",
    "\n",
    "        #brightness distortion\n",
    "        if random.randrange(2):\n",
    "            _convert(image, beta=random.uniform(-32, 32))\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        #saturation distortion\n",
    "        if random.randrange(2):\n",
    "            _convert(image[:, :, 1], alpha=random.uniform(0.5, 1.5))\n",
    "\n",
    "        #hue distortion\n",
    "        if random.randrange(2):\n",
    "            tmp = image[:, :, 0].astype(int) + random.randint(-18, 18)\n",
    "            tmp %= 180\n",
    "            image[:, :, 0] = tmp\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        #contrast distortion\n",
    "        if random.randrange(2):\n",
    "            _convert(image, alpha=random.uniform(0.5, 1.5))\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def _expand(image, boxes, fill, p):\n",
    "    if random.randrange(2):\n",
    "        return image, boxes\n",
    "\n",
    "    height, width, depth = image.shape\n",
    "\n",
    "    scale = random.uniform(1, p)\n",
    "    w = int(scale * width)\n",
    "    h = int(scale * height)\n",
    "\n",
    "    left = random.randint(0, w - width)\n",
    "    top = random.randint(0, h - height)\n",
    "\n",
    "    boxes_t = boxes.copy()\n",
    "    boxes_t[:, :2] += (left, top)\n",
    "    boxes_t[:, 2:] += (left, top)\n",
    "    expand_image = np.empty(\n",
    "        (h, w, depth),\n",
    "        dtype=image.dtype)\n",
    "    expand_image[:, :] = fill\n",
    "    expand_image[top:top + height, left:left + width] = image\n",
    "    image = expand_image\n",
    "\n",
    "    return image, boxes_t\n",
    "\n",
    "\n",
    "def _mirror(image, boxes):\n",
    "    _, width, _ = image.shape\n",
    "    if random.randrange(2):\n",
    "        image = image[:, ::-1]\n",
    "        boxes = boxes.copy()\n",
    "        boxes[:, 0::2] = width - boxes[:, 2::-2]\n",
    "    return image, boxes\n",
    "\n",
    "\n",
    "def _pad_to_square(image, rgb_mean, pad_image_flag):\n",
    "    if not pad_image_flag:\n",
    "        return image\n",
    "    height, width, _ = image.shape\n",
    "    long_side = max(width, height)\n",
    "    image_t = np.empty((long_side, long_side, 3), dtype=image.dtype)\n",
    "    image_t[:, :] = rgb_mean\n",
    "    image_t[0:0 + height, 0:0 + width] = image\n",
    "    return image_t\n",
    "\n",
    "\n",
    "def _resize_subtract_mean(image, insize, rgb_mean):\n",
    "    interp_methods = [cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_NEAREST, cv2.INTER_LANCZOS4]\n",
    "    interp_method = interp_methods[random.randrange(5)]\n",
    "    image = cv2.resize(image, (insize, insize), interpolation=interp_method)\n",
    "    image = image.astype(np.float32)\n",
    "    image -= rgb_mean\n",
    "    return image.transpose(2, 0, 1)\n",
    "\n",
    "\n",
    "class preproc(object):\n",
    "\n",
    "    def __init__(self, img_dim, rgb_means):\n",
    "        self.img_dim = img_dim\n",
    "        self.rgb_means = rgb_means\n",
    "\n",
    "    def __call__(self, image, targets):\n",
    "        assert targets.shape[0] > 0, \"this image does not have gt\"\n",
    "\n",
    "        boxes = targets[:, :-1].copy()\n",
    "        labels = targets[:, -1].copy()\n",
    "\n",
    "        #image_t = _distort(image)\n",
    "        #image_t, boxes_t = _expand(image_t, boxes, self.cfg['rgb_mean'], self.cfg['max_expand_ratio'])\n",
    "        #image_t, boxes_t, labels_t = _crop(image_t, boxes, labels, self.img_dim, self.rgb_means)\n",
    "        image_t, boxes_t, labels_t, pad_image_flag = _crop(image, boxes, labels, self.img_dim)\n",
    "        image_t = _distort(image_t)\n",
    "        image_t = _pad_to_square(image_t,self.rgb_means, pad_image_flag)\n",
    "        image_t, boxes_t = _mirror(image_t, boxes_t)\n",
    "        height, width, _ = image_t.shape\n",
    "        image_t = _resize_subtract_mean(image_t, self.img_dim, self.rgb_means)\n",
    "        boxes_t[:, 0::2] /= width\n",
    "        boxes_t[:, 1::2] /= height\n",
    "\n",
    "        labels_t = np.expand_dims(labels_t, 1)\n",
    "        targets_t = np.hstack((boxes_t, labels_t))\n",
    "\n",
    "        return image_t, targets_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import sys\n",
    "import torch.utils.data as data\n",
    "import cv2\n",
    "import numpy as np\n",
    "if sys.version_info[0] == 2:\n",
    "    import xml.etree.cElementTree as ET\n",
    "else:\n",
    "    import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "WIDER_CLASSES = ( '__background__', 'face')\n",
    "\n",
    "\n",
    "class AnnotationTransform(object):\n",
    "\n",
    "    \"\"\"Transforms a VOC annotation into a Tensor of bbox coords and label index\n",
    "    Initilized with a dictionary lookup of classnames to indexes\n",
    "\n",
    "    Arguments:\n",
    "        class_to_ind (dict, optional): dictionary lookup of classnames -> indexes\n",
    "            (default: alphabetic indexing of VOC's 20 classes)\n",
    "        keep_difficult (bool, optional): keep difficult instances or not\n",
    "            (default: False)\n",
    "        height (int): height\n",
    "        width (int): width\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, class_to_ind=None, keep_difficult=True):\n",
    "        self.class_to_ind = class_to_ind or dict(\n",
    "            zip(WIDER_CLASSES, range(len(WIDER_CLASSES))))\n",
    "        self.keep_difficult = keep_difficult\n",
    "\n",
    "    def __call__(self, target):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            target (annotation) : the target annotation to be made usable\n",
    "                will be an ET.Element\n",
    "        Returns:\n",
    "            a list containing lists of bounding boxes  [bbox coords, class name]\n",
    "        \"\"\"\n",
    "        res = np.empty((0, 5))\n",
    "        for obj in target.iter('object'):\n",
    "            difficult = int(obj.find('difficult').text) == 1\n",
    "            if not self.keep_difficult and difficult:\n",
    "                continue\n",
    "            name = obj.find('name').text.lower().strip()\n",
    "            bbox = obj.find('bndbox')\n",
    "\n",
    "            pts = ['xmin', 'ymin', 'xmax', 'ymax']\n",
    "            bndbox = []\n",
    "            for i, pt in enumerate(pts):\n",
    "                cur_pt = int(bbox.find(pt).text)\n",
    "                bndbox.append(cur_pt)\n",
    "            label_idx = self.class_to_ind[name]\n",
    "            bndbox.append(label_idx)\n",
    "            res = np.vstack((res, bndbox))  # [xmin, ymin, xmax, ymax, label_ind]\n",
    "        return res\n",
    "\n",
    "\n",
    "class VOCDetection(data.Dataset):\n",
    "\n",
    "    \"\"\"VOC Detection Dataset Object\n",
    "\n",
    "    input is image, target is annotation\n",
    "\n",
    "    Arguments:\n",
    "        root (string): filepath to WIDER folder\n",
    "        target_transform (callable, optional): transformation to perform on the\n",
    "            target `annotation`\n",
    "            (eg: take in caption string, return tensor of word indices)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, preproc=None, target_transform=None):\n",
    "        self.root = root\n",
    "        self.preproc = preproc\n",
    "        self.target_transform = target_transform\n",
    "        self._annopath = os.path.join(self.root, 'annotations', '%s')\n",
    "        self._imgpath = os.path.join(self.root, 'images', '%s')\n",
    "        self.ids = list()\n",
    "        with open(os.path.join(self.root, 'img_list.txt'), 'r') as f:\n",
    "          self.ids = [tuple(line.split()) for line in f]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.ids[index]\n",
    "        target = ET.parse(self._annopath % img_id[1]).getroot()\n",
    "        img = cv2.imread(self._imgpath % img_id[0], cv2.IMREAD_COLOR)\n",
    "        imgid = img_id[0]\n",
    "        height, width, _ = img.shape\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        if self.preproc is not None:\n",
    "            img, target = self.preproc(img, target)\n",
    "\n",
    "        return torch.from_numpy(img), target, imgid\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "\n",
    "def detection_collate(batch):\n",
    "    \"\"\"Custom collate fn for dealing with batches of images that have a different\n",
    "    number of associated object annotations (bounding boxes).\n",
    "\n",
    "    Arguments:\n",
    "        batch: (tuple) A tuple of tensor images and lists of annotations\n",
    "\n",
    "    Return:\n",
    "        A tuple containing:\n",
    "            1) (tensor) batch of images stacked on their 0 dim\n",
    "            2) (list of tensors) annotations for a given image are stacked on 0 dim\n",
    "    \"\"\"\n",
    "    targets = []\n",
    "    imgs = []\n",
    "    for _, sample in enumerate(batch):\n",
    "        for _, tup in enumerate(sample):\n",
    "            if torch.is_tensor(tup):\n",
    "                imgs.append(tup)\n",
    "            elif isinstance(tup, type(np.empty(0))):\n",
    "                annos = torch.from_numpy(tup).float()\n",
    "                targets.append(annos)\n",
    "\n",
    "    return (torch.stack(imgs, 0), targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dim = 1024 # only 1024 is supported\n",
    "rgb_mean = (104, 117, 123) # bgr order\n",
    "num_classes = 2\n",
    "initial_lr = 1e-3\n",
    "gamma = 0.1\n",
    "max_epoch = 5\n",
    "training_dataset = './FaceBoxes.PyTorch/data/WIDER_FACE'\n",
    "resume_net = None\n",
    "resume_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VOCDetection(training_dataset, preproc(img_dim, rgb_mean), AnnotationTransform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ -8.,  -4.,   1.,  ...,  63.,  62.,  61.],\n",
       "          [ -8.,  -5.,  -1.,  ...,  63.,  62.,  61.],\n",
       "          [-11.,  -8.,  -6.,  ...,  63.,  62.,  61.],\n",
       "          ...,\n",
       "          [-26., -25., -25.,  ..., -72., -72., -72.],\n",
       "          [-27., -26., -28.,  ..., -72., -72., -72.],\n",
       "          [-27., -27., -29.,  ..., -72., -72., -72.]],\n",
       " \n",
       "         [[-18., -13.,  -9.,  ...,  58.,  58.,  58.],\n",
       "          [-19., -16., -12.,  ...,  58.,  58.,  58.],\n",
       "          [-23., -20., -17.,  ...,  58.,  58.,  58.],\n",
       "          ...,\n",
       "          [ -5.,  -5.,  -5.,  ..., -85., -85., -85.],\n",
       "          [ -6.,  -6.,  -9.,  ..., -85., -85., -85.],\n",
       "          [ -6.,  -7., -10.,  ..., -85., -85., -85.]],\n",
       " \n",
       "         [[-25., -20., -13.,  ...,  52.,  52.,  52.],\n",
       "          [-26., -22., -16.,  ...,  52.,  52.,  52.],\n",
       "          [-30., -26., -22.,  ...,  52.,  52.,  52.],\n",
       "          ...,\n",
       "          [-28., -28., -31.,  ..., -91., -91., -91.],\n",
       "          [-29., -30., -34.,  ..., -91., -91., -91.],\n",
       "          [-30., -31., -34.,  ..., -91., -91., -91.]]]),\n",
       " array([[0.23753666, 0.2771261 , 0.28739003, 0.33870968, 1.        ],\n",
       "        [0.03665689, 0.25513196, 0.1026393 , 0.3313783 , 1.        ],\n",
       "        [0.92228739, 0.40175953, 1.        , 0.55131965, 1.        ],\n",
       "        [0.74633431, 0.2771261 , 0.7829912 , 0.32844575, 1.        ],\n",
       "        [0.44721408, 0.23607038, 0.5058651 , 0.30205279, 1.        ],\n",
       "        [0.37829912, 0.26392962, 0.42815249, 0.32258065, 1.        ]]),\n",
       " '0--Parade/0_Parade_marchingband_1_100.jpg')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##every time running the following command will give a different result(matrix),but the img name remains the same\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "batch_iterator = data.DataLoader(dataset,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fb41910e780>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[  4.,   4.,   2.,  ...,   3.,   3.,   4.],\n",
      "          [  4.,   4.,   2.,  ...,   3.,   3.,   4.],\n",
      "          [  2.,   2.,   3.,  ...,   4.,   4.,   5.],\n",
      "          ...,\n",
      "          [-23., -23., -27.,  ..., -76., -76., -76.],\n",
      "          [-23., -23., -27.,  ..., -76., -76., -76.],\n",
      "          [-22., -22., -21.,  ..., -76., -76., -76.]],\n",
      "\n",
      "         [[ 18.,  18.,  16.,  ...,  -9.,  -9.,  -8.],\n",
      "          [ 18.,  18.,  16.,  ...,  -9.,  -9.,  -8.],\n",
      "          [ 16.,  16.,  15.,  ...,  -8.,  -8.,  -7.],\n",
      "          ...,\n",
      "          [ 19.,  19.,  15.,  ..., -89., -89., -89.],\n",
      "          [ 19.,  19.,  15.,  ..., -89., -89., -89.],\n",
      "          [ 21.,  21.,  22.,  ..., -89., -89., -89.]],\n",
      "\n",
      "         [[ 19.,  19.,  19.,  ...,  -6.,  -6.,  -5.],\n",
      "          [ 19.,  19.,  19.,  ...,  -6.,  -6.,  -5.],\n",
      "          [ 15.,  15.,  17.,  ...,  -5.,  -5.,  -4.],\n",
      "          ...,\n",
      "          [ -9.,  -9., -13.,  ..., -95., -95., -95.],\n",
      "          [ -9.,  -9., -13.,  ..., -95., -95., -95.],\n",
      "          [ -8.,  -8.,  -7.,  ..., -95., -95., -95.]]]]), tensor([[[0.1495, 0.0544, 0.2155, 0.1359, 1.0000],\n",
      "         [0.8233, 0.0544, 0.8718, 0.1223, 1.0000],\n",
      "         [0.4272, 0.0000, 0.5049, 0.0874, 1.0000],\n",
      "         [0.3359, 0.0369, 0.4019, 0.1146, 1.0000]]], dtype=torch.float64), ('0--Parade/0_Parade_marchingband_1_100.jpg',)]\n"
     ]
    }
   ],
   "source": [
    "for r, rec in enumerate(batch_iterator):\n",
    "    print (rec)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
