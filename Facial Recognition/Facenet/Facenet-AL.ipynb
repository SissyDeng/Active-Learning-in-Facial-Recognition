{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The code below is for the SubSampler Class, which we will use to train the model using only a fraction of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data.sampler as samplers\n",
    "\n",
    "class SubsetSampler(samplers.Sampler):\n",
    "\n",
    "    def __init__(self, indices):\n",
    "        self.indices = indices\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in range(len(self.indices)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Bke87ExY8UB"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovOy2wavxmgV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet34\n",
    "\n",
    "\n",
    "class FaceNetModel(nn.Module):\n",
    "    def __init__(self, embedding_size, num_classes, pretrained=False):\n",
    "        super (FaceNetModel, self).__init__()\n",
    "        \n",
    "        self.model = resnet34(pretrained)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.model.fc = nn.Linear(8192, self.embedding_size)\n",
    "        self.model.classifier = nn.Linear(self.embedding_size, num_classes)\n",
    "    \n",
    "    \n",
    "    def l2_norm(self, input):\n",
    "        input_size = input.size()\n",
    "        buffer = torch.pow(input, 2)\n",
    "        normp = torch.sum(buffer, 1).add_(1e-10)\n",
    "        norm = torch.sqrt(normp)\n",
    "        _output = torch.div(input, norm.view(-1, 1).expand_as(input))\n",
    "        output = _output.view(input_size)\n",
    "    \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.model.fc(x)\n",
    "        \n",
    "        self.features = self.l2_norm(x)\n",
    "        # Multiply by alpha = 10 as suggested in https://arxiv.org/pdf/1703.09507.pdf\n",
    "        alpha         = 10\n",
    "        self.features = self.features*alpha\n",
    "        \n",
    "        return self.features\n",
    "    \n",
    "    \n",
    "    def forward_classifier(self, x):\n",
    "        features = self.forward(x)\n",
    "        res      = self.model.classifier(features)\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7DhykJaEbsZg"
   },
   "source": [
    "# Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YxhzZMQEn7FN"
   },
   "outputs": [],
   "source": [
    "def train_valid(model, optimizer, scheduler, epoch, dataloaders, data_size, margin):\n",
    "    \n",
    "    for phase in ['train', 'valid']:\n",
    "\n",
    "        labels, distances = [], []\n",
    "        triplet_loss_sum  = 0.0\n",
    "\n",
    "        if phase == 'train':\n",
    "            optimizer.zero_grad()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        for batch_idx, batch_sample in enumerate(dataloaders[phase]):\n",
    "            \n",
    "            anc_img = batch_sample['anc_img'].to(device)\n",
    "            pos_img = batch_sample['pos_img'].to(device)\n",
    "            neg_img = batch_sample['neg_img'].to(device)\n",
    "        \n",
    "            pos_cls = batch_sample['pos_class'].to(device)\n",
    "            neg_cls = batch_sample['neg_class'].to(device)\n",
    "                            \n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "            \n",
    "                # anc_embed, pos_embed and neg_embed are encoding(embedding) of image\n",
    "                anc_embed, pos_embed, neg_embed = model(anc_img), model(pos_img), model(neg_img)\n",
    "            \n",
    "                # choose the hard negatives only for \"training\"\n",
    "                pos_dist = l2_dist.forward(anc_embed, pos_embed)\n",
    "                neg_dist = l2_dist.forward(anc_embed, neg_embed)\n",
    "            \n",
    "                all = (neg_dist - pos_dist < margin).cpu().numpy().flatten()\n",
    "                if phase == 'train':\n",
    "                    hard_triplets = np.where(all == 1)\n",
    "                    if len(hard_triplets[0]) == 0:\n",
    "                        continue\n",
    "                else:\n",
    "                    hard_triplets = np.where(all >= 0)\n",
    "                \n",
    "                anc_hard_embed = anc_embed[hard_triplets].to(device)\n",
    "                pos_hard_embed = pos_embed[hard_triplets].to(device)\n",
    "                neg_hard_embed = neg_embed[hard_triplets].to(device)\n",
    "            \n",
    "                anc_hard_img  = anc_img[hard_triplets].to(device)\n",
    "                pos_hard_img  = pos_img[hard_triplets].to(device)\n",
    "                neg_hard_img  = neg_img[hard_triplets].to(device)\n",
    "            \n",
    "                pos_hard_cls  = pos_cls[hard_triplets].to(device)\n",
    "                neg_hard_cls  = neg_cls[hard_triplets].to(device)\n",
    "            \n",
    "                anc_img_pred  = model.forward_classifier(anc_hard_img).to(device)\n",
    "                pos_img_pred  = model.forward_classifier(pos_hard_img).to(device)\n",
    "                neg_img_pred  = model.forward_classifier(neg_hard_img).to(device)\n",
    "            \n",
    "                triplet_loss  = TripletLoss(margin).forward(anc_hard_embed, pos_hard_embed, neg_hard_embed).to(device)\n",
    "        \n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    triplet_loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "                dists = l2_dist.forward(anc_embed, pos_embed)\n",
    "                distances.append(dists.data.cpu().numpy())\n",
    "                labels.append(np.ones(dists.size(0))) \n",
    "\n",
    "                dists = l2_dist.forward(anc_embed, neg_embed)\n",
    "                distances.append(dists.data.cpu().numpy())\n",
    "                labels.append(np.zeros(dists.size(0)))\n",
    "                \n",
    "                loss_sum_batch = triplet_loss.item()\n",
    "                triplet_loss_sum += triplet_loss.item()\n",
    "                          \n",
    "        avg_triplet_loss = triplet_loss_sum / data_size[phase]\n",
    "        labels = np.array([sublabel for label in labels for sublabel in label])\n",
    "        distances = np.array([subdist for dist in distances for subdist in dist])\n",
    "        \n",
    "#         if phase == 'train':\n",
    "#             torch.save({'epoch': epoch,\n",
    "#                   'state_dict': model.state_dict()},\n",
    "#                   train_root_dir+'log/checkpoint_epoch{}.pth'.format(epoch))\n",
    "            \n",
    "        if phase == 'valid':\n",
    "            return distances, labels, avg_triplet_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2EsrDVIPb0xf"
   },
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5yWX4JA0b3As"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TripletFaceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, csv_name, num_triplets, transform = None):\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.df = pd.read_csv(csv_name)\n",
    "        self.num_triplets = num_triplets\n",
    "        self.transform = transform\n",
    "        self.training_triplets = self.generate_triplets(self.df, self.num_triplets)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_triplets(df, num_triplets):\n",
    "        \n",
    "        def make_dictionary_for_face_class(df):\n",
    "\n",
    "            '''\n",
    "              - face_classes = {'class0': [class0_id0, ...], 'class1': [class1_id0, ...], ...}\n",
    "            '''\n",
    "            face_classes = dict()\n",
    "            for idx, label in enumerate(df['class']):\n",
    "                if label not in face_classes:\n",
    "                    face_classes[label] = []\n",
    "                face_classes[label].append(df.iloc[idx, 0])\n",
    "            return face_classes\n",
    "        \n",
    "        triplets = []\n",
    "        classes = df['class'].unique()\n",
    "        face_classes = make_dictionary_for_face_class(df)\n",
    "         \n",
    "        for _ in range(num_triplets):\n",
    "\n",
    "            '''\n",
    "              - randomly choose anchor, positive and negative images for triplet loss\n",
    "              - anchor and positive images in pos_class\n",
    "              - negative image in neg_class\n",
    "              - at least, two images needed for anchor and positive images in pos_class\n",
    "              - negative image should have different class as anchor and positive images by definition\n",
    "            '''\n",
    "        \n",
    "            pos_class = np.random.choice(classes)\n",
    "            neg_class = np.random.choice(classes)\n",
    "            while len(face_classes[pos_class]) < 2:\n",
    "                pos_class = np.random.choice(classes)\n",
    "            while pos_class == neg_class:\n",
    "                neg_class = np.random.choice(classes)\n",
    "            \n",
    "            pos_name = df.loc[df['class'] == pos_class, 'name'].values[0]\n",
    "            neg_name = df.loc[df['class'] == neg_class, 'name'].values[0]\n",
    "\n",
    "            if len(face_classes[pos_class]) == 2:\n",
    "                ianc, ipos = np.random.choice(2, size = 2, replace = False)\n",
    "            else:\n",
    "                ianc = np.random.randint(0, len(face_classes[pos_class]))\n",
    "                ipos = np.random.randint(0, len(face_classes[pos_class]))\n",
    "                while ianc == ipos:\n",
    "                    ipos = np.random.randint(0, len(face_classes[pos_class]))\n",
    "            ineg = np.random.randint(0, len(face_classes[neg_class]))\n",
    "            \n",
    "            triplets.append([face_classes[pos_class][ianc], face_classes[pos_class][ipos], face_classes[neg_class][ineg], \n",
    "                             pos_class, neg_class, pos_name, neg_name])\n",
    "        \n",
    "        return triplets\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        anc_id, pos_id, neg_id, pos_class, neg_class, pos_name, neg_name = self.training_triplets[idx]\n",
    "        \n",
    "        anc_img   = os.path.join(self.root_dir, str(pos_name), str(anc_id) + '.jpg')\n",
    "        pos_img   = os.path.join(self.root_dir, str(pos_name), str(pos_id) + '.jpg')\n",
    "        neg_img   = os.path.join(self.root_dir, str(neg_name), str(neg_id) + '.jpg')\n",
    "        \n",
    "        anc_img   = io.imread(anc_img)\n",
    "        pos_img   = io.imread(pos_img)\n",
    "        neg_img   = io.imread(neg_img)\n",
    "\n",
    "        pos_class = torch.from_numpy(np.array([pos_class]).astype('long'))\n",
    "        neg_class = torch.from_numpy(np.array([neg_class]).astype('long'))\n",
    "        \n",
    "        sample = {'anc_img': anc_img, 'pos_img': pos_img, 'neg_img': neg_img, 'pos_class': pos_class, 'neg_class': neg_class}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['anc_img'] = self.transform(sample['anc_img'])\n",
    "            sample['pos_img'] = self.transform(sample['pos_img'])\n",
    "            sample['neg_img'] = self.transform(sample['neg_img'])\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.training_triplets)\n",
    "    \n",
    "\n",
    "def get_dataloader(train_root_dir,     valid_root_dir, \n",
    "          train_csv_name,     valid_csv_name, \n",
    "          num_train_triplets, num_valid_triplets):\n",
    "    \n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5])]),\n",
    "        'valid': transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5])])}\n",
    "\n",
    "    face_dataset = {\n",
    "        'train' : TripletFaceDataset(root_dir     = train_root_dir,\n",
    "                                  csv_name = train_csv_name,\n",
    "                                  num_triplets = num_train_triplets,\n",
    "                                  transform = data_transforms['train']),\n",
    "        'valid' : TripletFaceDataset(root_dir     = valid_root_dir,\n",
    "                                  csv_name = valid_csv_name,\n",
    "                                  num_triplets = num_valid_triplets,\n",
    "                                  transform = data_transforms['valid'])}\n",
    "\n",
    "    return face_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8EvI-dw0cA0c"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OBlfNoBtcFrd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.nn.modules.distance import PairwiseDistance\n",
    "\n",
    "\n",
    "class TripletLoss(Function):\n",
    "    \n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.pdist  = PairwiseDistance(2)\n",
    "        \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        pos_dist = self.pdist.forward(anchor, positive)\n",
    "        neg_dist = self.pdist.forward(anchor, negative)\n",
    "        \n",
    "        hinge_dist = torch.clamp(self.margin + pos_dist - neg_dist, min = 0.0)\n",
    "        loss = torch.mean(hinge_dist)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nsdil8R4cNjR"
   },
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ry3RP7hHcRro"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy import interpolate\n",
    "\n",
    "\n",
    "def evaluate(distances, labels, nrof_folds=10):\n",
    "    # Calculate evaluation metrics\n",
    "    thresholds = np.arange(0, 30, 0.01)\n",
    "    tpr, fpr, accuracy = calculate_roc(thresholds, distances,\n",
    "        labels, nrof_folds=nrof_folds)\n",
    "    thresholds = np.arange(0, 30, 0.001)\n",
    "    val, val_std, far = calculate_val(thresholds, distances,\n",
    "        labels, 1e-3, nrof_folds=nrof_folds)\n",
    "    return tpr, fpr, accuracy, val, val_std, far\n",
    "\n",
    "\n",
    "def calculate_roc(thresholds, distances, labels, nrof_folds=10):\n",
    "\n",
    "    nrof_pairs = min(len(labels), len(distances))\n",
    "    nrof_thresholds = len(thresholds)\n",
    "    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n",
    "\n",
    "    tprs = np.zeros((nrof_folds,nrof_thresholds))\n",
    "    fprs = np.zeros((nrof_folds,nrof_thresholds))\n",
    "    accuracy = np.zeros((nrof_folds))\n",
    "\n",
    "    indices = np.arange(nrof_pairs)\n",
    "\n",
    "    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n",
    "\n",
    "        # Find the best threshold for the fold\n",
    "        acc_train = np.zeros((nrof_thresholds))\n",
    "        for threshold_idx, threshold in enumerate(thresholds):\n",
    "            _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, distances[train_set], labels[train_set])\n",
    "        best_threshold_index = np.argmax(acc_train)\n",
    "        for threshold_idx, threshold in enumerate(thresholds):\n",
    "            tprs[fold_idx,threshold_idx], fprs[fold_idx,threshold_idx], _ = calculate_accuracy(threshold, distances[test_set], labels[test_set])\n",
    "        _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], distances[test_set], labels[test_set])\n",
    "\n",
    "        tpr = np.mean(tprs,0)\n",
    "        fpr = np.mean(fprs,0)\n",
    "    return tpr, fpr, accuracy\n",
    "\n",
    "\n",
    "def calculate_accuracy(threshold, dist, actual_issame):\n",
    "    predict_issame = np.less(dist, threshold)\n",
    "    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n",
    "    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n",
    "    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n",
    "    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n",
    "\n",
    "    tpr = 0 if (tp+fn==0) else float(tp) / float(tp+fn)\n",
    "    fpr = 0 if (fp+tn==0) else float(fp) / float(fp+tn)\n",
    "    acc = float(tp+tn)/dist.size\n",
    "    return tpr, fpr, acc\n",
    "\n",
    "\n",
    "def calculate_val(thresholds, distances, labels, far_target=1e-3, nrof_folds=10):\n",
    "    nrof_pairs = min(len(labels), len(distances))\n",
    "    nrof_thresholds = len(thresholds)\n",
    "    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n",
    "\n",
    "    val = np.zeros(nrof_folds)\n",
    "    far = np.zeros(nrof_folds)\n",
    "\n",
    "    indices = np.arange(nrof_pairs)\n",
    "\n",
    "    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n",
    "\n",
    "        # Find the threshold that gives FAR = far_target\n",
    "        far_train = np.zeros(nrof_thresholds)\n",
    "        for threshold_idx, threshold in enumerate(thresholds):\n",
    "            _, far_train[threshold_idx] = calculate_val_far(threshold, distances[train_set], labels[train_set])\n",
    "        if np.max(far_train)>=far_target:\n",
    "            f = interpolate.interp1d(far_train, thresholds, kind='slinear')\n",
    "            threshold = f(far_target)\n",
    "        else:\n",
    "            threshold = 0.0\n",
    "\n",
    "        val[fold_idx], far[fold_idx] = calculate_val_far(threshold, distances[test_set], labels[test_set])\n",
    "\n",
    "    val_mean = np.mean(val)\n",
    "    far_mean = np.mean(far)\n",
    "    val_std = np.std(val)\n",
    "    return val_mean, val_std, far_mean\n",
    "\n",
    "\n",
    "def calculate_val_far(threshold, dist, actual_issame):\n",
    "    predict_issame = np.less(dist, threshold)\n",
    "    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\n",
    "    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n",
    "    n_same = np.sum(actual_issame)\n",
    "    n_diff = np.sum(np.logical_not(actual_issame))\n",
    "    if n_diff == 0:\n",
    "        n_diff = 1\n",
    "    if n_same == 0:\n",
    "        return 0,0\n",
    "    val = float(true_accept) / float(n_same)\n",
    "    far = float(false_accept) / float(n_diff)\n",
    "    return val, far\n",
    "\n",
    "\n",
    "def plot_roc(fpr,tpr,figure_name=\"roc.png\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.switch_backend('Agg')\n",
    "\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fig = plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='red',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='blue', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    fig.savefig(figure_name, dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iTLlYi7tl0dy"
   },
   "source": [
    "# AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tz5VDUQ0gq4E"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.nn.modules.distance import PairwiseDistance\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import random\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "num_classes = 5760\n",
    "num_train_triplets = 5760\n",
    "num_valid_triplets = 2000\n",
    "embedding_size = 128\n",
    "batch_size = 64 \n",
    "num_workers = 4\n",
    "learning_rate = 0.001\n",
    "margin = 0.5\n",
    "train_root_dir = './train/'\n",
    "valid_root_dir = './val/'\n",
    "train_csv_name = './train/lfw_train.csv'\n",
    "valid_csv_name = './val/lfw_valid.csv'\n",
    "num_steps = 9        \n",
    "nps = int(num_train_triplets / num_steps)\n",
    "\n",
    "device  = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "l2_dist = PairwiseDistance(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qN_p4F_BBaH7"
   },
   "source": [
    "- Let us show some of the training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# face_dataset = get_dataloader(train_root_dir,     valid_root_dir, \n",
    "#                               train_csv_name,     valid_csv_name, \n",
    "#                               num_train_triplets, num_valid_triplets)\n",
    "# dataloaders = {\n",
    "#         x: torch.utils.data.DataLoader(face_dataset[x], batch_size = batch_size, shuffle = False, num_workers = num_workers)\n",
    "#         for x in ['train', 'valid']}\n",
    "    \n",
    "# size = {x: len(face_dataset[x]) for x in ['train', 'valid']}\n",
    "\n",
    "# df = pd.read_csv(valid_csv_name) \n",
    "# classes = tuple(sorted(set(df['name']),key=list(df['name']).index))\n",
    "\n",
    "# # functions to show an image\n",
    "# def imshow(img):\n",
    "#     img = img / 2 + 0.5     # unnormalize\n",
    "#     npimg = img.numpy()\n",
    "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "#     plt.show()\n",
    "\n",
    "# # get some random training images\n",
    "# mydataiter = iter(dataloaders['valid'])\n",
    "# mydata = mydataiter.next()\n",
    "# myimages, mylabels = mydata['anc_img'], mydata['pos_class']\n",
    "# myimage, mylabel = myimages[0:4], mylabels[0:4]\n",
    "# # show images\n",
    "# imshow(torchvision.utils.make_grid(myimage))\n",
    "# # print labels\n",
    "# print(' '.join('%5s' % classes[mylabel[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bys3HKLRl7AQ"
   },
   "source": [
    "## AL Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mHBb_ebplyuk"
   },
   "outputs": [],
   "source": [
    "class ActiveStrategy(object):\n",
    "\n",
    "    def __init__(self, model, nsteps, margin, num_epochs, batch_size, num_workers):\n",
    "        self.model = model\n",
    "        self.nsteps = nsteps\n",
    "        self.margin = margin\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.face_dataset = get_dataloader(train_root_dir,     valid_root_dir,\n",
    "                                           train_csv_name,     valid_csv_name,\n",
    "                                           num_train_triplets, num_valid_triplets)\n",
    "        self.statsloader = torch.utils.data.DataLoader(self.face_dataset['train'], \n",
    "                                                       batch_size = self.batch_size, \n",
    "                                                       shuffle = False,\n",
    "                                                       num_workers = self.num_workers)\n",
    "        self.trainloader = torch.utils.data.DataLoader(self.face_dataset['train'], \n",
    "                                                       batch_size = self.batch_size, \n",
    "                                                       shuffle = False,\n",
    "                                                       num_workers = self.num_workers)\n",
    "        self.validloader = torch.utils.data.DataLoader(self.face_dataset['valid'], \n",
    "                                                       batch_size = self.batch_size, \n",
    "                                                       shuffle = False,\n",
    "                                                       num_workers = self.num_workers)\n",
    "        self.data_loader = {'train': self.trainloader, 'valid': self.validloader}\n",
    "        self.train_length = len(self.face_dataset['train'])\n",
    "        self.valid_length = len(self.face_dataset['valid'])\n",
    "        self.train_filter = [ i for i in range(self.train_length)]\n",
    "        self.valid_filter = [ i for i in range(self.valid_length)]\n",
    "        self.train_sampler = SubsetSampler(self.train_filter)\n",
    "        self.valid_sampler = SubsetSampler(self.valid_filter)\n",
    "        self.experiments = []\n",
    "\n",
    "    def incremental_supervised(self):\n",
    "        np.random.shuffle(self.train_filter)\n",
    "        \n",
    "    def train_test(self,data_size):\n",
    "        optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size = 50, gamma = 0.1)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            distances, labels, avg_triplet_loss = train_valid(self.model, optimizer, scheduler, epoch, self.data_loader, data_size, self.margin)\n",
    "            \n",
    "        print('Finished Training')\n",
    "        return distances, labels, avg_triplet_loss\n",
    "        \n",
    "        ##### Stats below ######  \n",
    "\n",
    "    def infer(self, sample):\n",
    "        sampler = SubsetSampler(sample)\n",
    "        dataloader = torch.utils.data.DataLoader(self.face_dataset['train'],\n",
    "                                                  shuffle=False,\n",
    "                                                  batch_size=1,\n",
    "                                                  num_workers=self.num_workers,\n",
    "                                                  sampler=sampler)\n",
    "        results = []\n",
    "        labels, distances = [], []\n",
    "        model.eval()\n",
    "        for r, rec in enumerate(dataloader):\n",
    "            anc_img = rec['anc_img'].to(device)\n",
    "            pos_img = rec['pos_img'].to(device)\n",
    "            neg_img = rec['neg_img'].to(device)\n",
    "        \n",
    "            pos_cls = rec['pos_class'].to(device)\n",
    "            neg_cls = rec['neg_class'].to(device)\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                anc_embed, pos_embed, neg_embed = model(anc_img), model(pos_img), model(neg_img)\n",
    "                \n",
    "                pos_dist = l2_dist.forward(anc_embed, pos_embed)\n",
    "                neg_dist = l2_dist.forward(anc_embed, neg_embed)\n",
    "                \n",
    "                all = (neg_dist - pos_dist < margin).cpu().numpy().flatten()\n",
    "                \n",
    "                hard_triplets = np.where(all >= 0)\n",
    "\n",
    "                anc_hard_embed = anc_embed[hard_triplets].to(device)\n",
    "                pos_hard_embed = pos_embed[hard_triplets].to(device)\n",
    "                neg_hard_embed = neg_embed[hard_triplets].to(device)\n",
    "            \n",
    "                anc_hard_img  = anc_img[hard_triplets].to(device)\n",
    "                pos_hard_img  = pos_img[hard_triplets].to(device)\n",
    "                neg_hard_img  = neg_img[hard_triplets].to(device)\n",
    "            \n",
    "                pos_hard_cls  = pos_cls[hard_triplets].to(device)\n",
    "                neg_hard_cls  = neg_cls[hard_triplets].to(device)\n",
    "            \n",
    "                anc_img_pred  = model.forward_classifier(anc_hard_img).to(device)\n",
    "                pos_img_pred  = model.forward_classifier(pos_hard_img).to(device)\n",
    "                neg_img_pred  = model.forward_classifier(neg_hard_img).to(device)\n",
    "            \n",
    "                triplet_loss  = TripletLoss(margin).forward(anc_hard_embed, pos_hard_embed, neg_hard_embed).to(device)\n",
    "                \n",
    "                dists1 = l2_dist.forward(anc_embed, pos_embed)\n",
    "                distances.append(dists1.data.cpu().numpy())\n",
    "                labels.append(np.ones(dists1.size(0))) \n",
    "\n",
    "                dists0 = l2_dist.forward(anc_embed, neg_embed)\n",
    "                distances.append(dists0.data.cpu().numpy())\n",
    "                labels.append(np.zeros(dists0.size(0)))\n",
    "                \n",
    "            results.append([r,triplet_loss.item(),dists1.data.cpu().numpy(),dists0.data.cpu().numpy()])\n",
    "        return results                \n",
    "    \n",
    "    def run_one(self, selected):\n",
    "        self.train_filter = selected\n",
    "        print(\"Training for {0} records:\".format(len(selected)))\n",
    "        self.train_sampler = SubsetSampler(self.train_filter)\n",
    "        self.trainloader = torch.utils.data.DataLoader(self.face_dataset['train'],\n",
    "                                                       shuffle=False,\n",
    "                                                       batch_size=self.batch_size,\n",
    "                                                       num_workers=1,\n",
    "                                                       sampler=self.train_sampler)\n",
    "        self.data_loader = {'train': self.trainloader, 'valid': self.validloader}\n",
    "        data_size = {'train':len(selected),'valid':self.valid_length}\n",
    "        distances, labels, avg_triplet_loss = self.train_test(data_size)\n",
    "        tpr, fpr, accuracy, val, val_std, far = evaluate(distances, labels)\n",
    "        print('Triplet Loss = {:.8f}'.format(avg_triplet_loss))\n",
    "        print('Accuracy of the network on the {0} test images: {1}%'.format(self.valid_length,100 * np.mean(accuracy)))\n",
    "        res = np.mean(accuracy)\n",
    "        return res\n",
    "    \n",
    "    def run_experiment(self, nsteps):\n",
    "        results = []\n",
    "        for n in range(1, nsteps+1):\n",
    "            nsamples = int(1.0 / nsteps * n * self.train_length)\n",
    "            print(\"Training for {0} samples:\".format(nsamples))\n",
    "            self.train_sampler = SubsetSampler(self.train_filter[:nsamples])\n",
    "            self.trainloader = torch.utils.data.DataLoader(self.face_dataset['train'],\n",
    "                                                           shuffle=False,\n",
    "                                                           batch_size=self.batch_size,\n",
    "                                                           num_workers=self.num_workers,\n",
    "                                                           sampler=self.train_sampler)\n",
    "            self.data_loader = {'train': self.trainloader, 'valid': self.validloader}\n",
    "            data_size = {x: nsamples for x in ['train', 'valid']}\n",
    "            distances, labels, avg_triplet_loss = self.train_test(data_size)\n",
    "            tpr, fpr, accuracy, val, val_std, far = evaluate(distances, labels)\n",
    "            print('Triplet Loss = {:.8f}'.format(avg_triplet_loss))\n",
    "            print('Accuracy of the network on the {0} test images: {1}%'.format(self.valid_length,100 * np.mean(accuracy)))\n",
    "            results.append(np.mean(accuracy))\n",
    "        return results\n",
    "    \n",
    "    def run_ConfidenceAL(self, qStrategy, nsteps, maximum):\n",
    "    \n",
    "        results = []\n",
    "    \n",
    "        unlabeled = [i for i in range(self.train_length)]\n",
    "        labeled   = []\n",
    "        l = []\n",
    "\n",
    "        to_be_labeled = random.sample(unlabeled, int(nps))\n",
    "        l.append(len(to_be_labeled))\n",
    "        \n",
    "        unlabeled = list(set(unlabeled)-set(to_be_labeled))\n",
    "        myres = self.run_one(to_be_labeled)\n",
    "        results.append(myres)\n",
    "    \n",
    "        for n in range(1, nsteps):\n",
    "            myResults = self.infer(unlabeled)\n",
    "            to_be_labeled.extend(qStrategy(myResults, int(maximum/nsteps))) # updating function\n",
    "            unlabeled = list(set(unlabeled)-set(to_be_labeled))\n",
    "            myres = self.run_one(to_be_labeled)\n",
    "            results.append(myres)\n",
    "            l.append(len(to_be_labeled))\n",
    "        \n",
    "        return results,l\n",
    "    \n",
    "    def run_StreamingAL(self, qStrategy, nsteps, maximum):\n",
    "    \n",
    "        results = []\n",
    "        stepSizes = []\n",
    "    \n",
    "        unlabeled = [i for i in range(self.train_length)]\n",
    "        labeled   = []\n",
    "\n",
    "        to_be_labeled = random.sample(unlabeled, int(nps))\n",
    "        unlabeled = list(set(unlabeled)-set(to_be_labeled))\n",
    "        myres = self.run_one(to_be_labeled)\n",
    "        results.append(myres)\n",
    "        stepSizes.append(len(to_be_labeled))\n",
    "    \n",
    "        for n in range(1, nsteps):\n",
    "            myResults = self.infer(unlabeled)\n",
    "            to_be_labeled.extend( qStrategy(myResults) ) # updating function\n",
    "            if (len(to_be_labeled) > maximum):\n",
    "                break\n",
    "            unlabeled = list(set(unlabeled)-set(to_be_labeled))\n",
    "            myres = self.run_one(to_be_labeled)\n",
    "            results.append(myres)\n",
    "            stepSizes.append(len(to_be_labeled))\n",
    "        \n",
    "        return results, stepSizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RkjiZtc8mGiK"
   },
   "source": [
    "## Active learning Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hpMsCAfiqoKR"
   },
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y1gRGwasmLG6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 640 samples:\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = FaceNetModel(embedding_size = embedding_size, num_classes = num_classes).to(device)\n",
    "myActiveStrategy = ActiveStrategy(model, num_steps, margin, num_epochs, batch_size, num_workers)\n",
    "\n",
    "myActiveStrategy.incremental_supervised()\n",
    "mySupervised = myActiveStrategy.run_experiment(num_steps)\n",
    "    \n",
    "# plt.plot([nps*i for i in range(1, num_steps + 1)], mySupervised, '--b', marker=\"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save results\n",
    "# df = pd.DataFrame(mySupervised)\n",
    "# df.to_csv('data.csv',index=False)\n",
    "# plt.plot([nps*i for i in range(1, num_steps + 1)], mySupervised, '--b', marker=\"o\")\n",
    "# plt.savefig(\"mySupervised.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mCCrH_I_rew7"
   },
   "source": [
    "### Uncertainty-Based Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "unlabeled = [i for i in range(num_train_triplets)]\n",
    "labeled   = []\n",
    "\n",
    "to_be_labeled = random.sample(unlabeled, nps) # generate a random sample of the data, size nps (defined previously)\n",
    "to_be_labeled_set = set(to_be_labeled)\n",
    "\n",
    "unlabeled = list(filter(lambda x: x not in to_be_labeled_set, unlabeled)) # update the unlabeled array once you defined to_be_labeled\n",
    "\n",
    "model1 = FaceNetModel(embedding_size = embedding_size, num_classes = num_classes).to(device)\n",
    "myActiveStrategy1 = ActiveStrategy(model1, num_steps, margin, num_epochs, batch_size, num_workers)\n",
    "myActiveStrategy1.run_one(to_be_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "myResults = myActiveStrategy1.infer(unlabeled)\n",
    "# myResults[:10]\n",
    "\n",
    "# Below, we update the to_be_labeled and unlabeled arrays, in preparation for the next loop...\n",
    "# the strategy is to select the records that were inferred with the lowest confidence in the previous loop...\n",
    "sorted_by_conf = sorted(myResults, key=lambda x: float(x[2] - x[3]),reverse = True)[:nps] # fill here; remember that the size of the array will be 'nps'\n",
    "\n",
    "\n",
    "# update to_be_labeled:\n",
    "to_be_labeled.extend([row[0] for row in sorted_by_conf])\n",
    "to_be_labeled_set = set(to_be_labeled)\n",
    "\n",
    "unlabeled = list(filter(lambda x: x not in to_be_labeled_set, unlabeled)) # update unlabeled\n",
    "\n",
    "# And run one more loop...\n",
    "accuracy = myActiveStrategy1.run_one(to_be_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing the distribution of the confidence level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# confidence level on triplet loss: d(a,p)-d(a,n)\n",
    "inferred_CL = [float(c[2] - c[3]) for c in myResults]\n",
    "print(inferred_CL[:10])\n",
    "print(len(inferred_CL))\n",
    "\n",
    "n, bins, patches = plt.hist(x=inferred_CL, bins=30, color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Difference')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Difference Level')\n",
    "maxfreq = n.max()\n",
    "\n",
    "plt.ylim(top=(np.ceil(maxfreq / 10) * 10 * 1.05) if maxfreq % 10 else (maxfreq + 10) * 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# confidence level on triplet loss: max(d(a,p)-d(a,n)+margin,0)\n",
    "inferred_CL = [float(c[1]) for c in myResults]\n",
    "print(inferred_CL[:10])\n",
    "print(len(inferred_CL))\n",
    "\n",
    "n1, bins, patches = plt.hist(x=inferred_CL, bins=30, color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Triplet Loss')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Triplet Loss Level')\n",
    "maxfreq1 = n1.max()\n",
    "\n",
    "plt.ylim(top=(np.ceil(maxfreq1 / 10) * 10 * 1.05) if maxfreq1 % 10 else (maxfreq1 + 10) * 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(myResults)\n",
    "# df.to_csv('results.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mySupervised = pd.read_csv(filepath_or_buffer='data.csv')\n",
    "# mySupervised = np.array(mySupervised).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZtN9VjJ0rfHv"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model2 = FaceNetModel(embedding_size = embedding_size, num_classes = num_classes).to(device)\n",
    "myActiveStrategy2 = ActiveStrategy(model2, num_steps, margin, num_epochs, batch_size, num_workers)\n",
    "\n",
    "def confidenceAL(nsteps, stepSize):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    unlabeled = [i for i in range(num_train_triplets)]\n",
    "    labeled   = []\n",
    "    \n",
    "    to_be_labeled = random.sample(unlabeled, stepSize)\n",
    "    to_be_labeled_set = set(to_be_labeled)\n",
    "    unlabeled = list(filter(lambda x: x not in to_be_labeled_set, unlabeled))\n",
    "    \n",
    "    myres = myActiveStrategy2.run_one(to_be_labeled)\n",
    "    results.append(myres)\n",
    "    \n",
    "    for n in range(1, nsteps):\n",
    "        myResults = myActiveStrategy2.infer(unlabeled)\n",
    "        sorted_by_conf = sorted(myResults, key=lambda x: x[1], reverse=True)[:stepSize] \n",
    "\n",
    "        # update to_be_labeled:\n",
    "        to_be_labeled.extend([row[0] for row in sorted_by_conf])\n",
    "        to_be_labeled_set = set(to_be_labeled)\n",
    "        unlabeled = list(filter(lambda x: x not in to_be_labeled_set, unlabeled)) # update unlabeled\n",
    "\n",
    "        myres = myActiveStrategy2.run_one(to_be_labeled)\n",
    "        results.append(myres)\n",
    "        \n",
    "    return results\n",
    " \n",
    "myConfidenceAL = confidenceAL(num_steps, nps)\n",
    "# plt.plot([nps*i for i in range(1, num_steps + 1)], mySupervised,    '--b',\n",
    "#          [nps*i for i in range(1, num_steps + 1)], myConfidenceAL,  '--g',\n",
    "#          marker = \"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence level based strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model3 = FaceNetModel(embedding_size = embedding_size, num_classes = num_classes).to(device)\n",
    "myActiveStrategy3 = ActiveStrategy(model3, num_steps, margin, num_epochs, batch_size, num_workers)\n",
    "\n",
    "def update_function(inferred_res, nRec):\n",
    "    ranked = sorted(inferred_res, key=lambda x: float(x[2] - x[3]), reverse=True)[:nRec] # Sort and select the best nRec elements\n",
    "    selected = [rec[0] for rec in ranked]\n",
    "    \n",
    "    return selected\n",
    "    \n",
    "myConfidenceAL2,_ = myActiveStrategy3.run_ConfidenceAL(update_function, num_steps, num_train_triplets)\n",
    "# plt.plot([nps*i for i in range(1, num_steps + 1)], mySupervised,    '--b',\n",
    "#          [nps*i for i in range(1, num_steps + 1)], myConfidenceAL,  '--r',\n",
    "#          [nps*i for i in range(1, num_steps + 1)], myConfidenceAL2, '--g',\n",
    "#          marker=\"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Better Confidence Level-Based Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model4 = FaceNetModel(embedding_size = embedding_size, num_classes = num_classes).to(device)\n",
    "myActiveStrategy4 = ActiveStrategy(model4, num_steps, margin, num_epochs, batch_size, num_workers)\n",
    "\n",
    "def update_CL_improved(inferred_res, nRec, nFilter=200): # <--- we can play with nFilter value later\n",
    "    ranked = sorted(inferred_res, key=lambda x: x[1])[nFilter:]\n",
    "    selected = [rec[0] for rec in ranked[:nRec]]\n",
    "    \n",
    "    return selected\n",
    "\n",
    "myConfidenceAL3,length_labeled = myActiveStrategy4.run_ConfidenceAL(update_CL_improved, num_steps, num_train_triplets)\n",
    "# plt.plot([nps*i for i in range(1, num_steps + 1)], mySupervised,    '--b',\n",
    "#          [nps*i for i in range(1, num_steps + 1)], myConfidenceAL2, '--g',\n",
    "#          length_labeled, myConfidenceAL3, '--p',\n",
    "#          marker=\"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Approach with Uncertainty-Based Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model5 = FaceNetModel(embedding_size = embedding_size, num_classes = num_classes).to(device)\n",
    "myActiveStrategy5 = ActiveStrategy(model5, num_steps, margin, num_epochs, batch_size, num_workers)\n",
    "\n",
    "def update_streaming_perc(inferred_res, threshold=95): # <-- here, threshold is the amount of data we want to keep\n",
    "    perc = np.percentile([x[1] for x in inferred_res], threshold)\n",
    "    print(perc)\n",
    "    next_loop = sorted(x[0] for x in inferred_res if float(x[1]) > perc)\n",
    "    \n",
    "    return next_loop\n",
    "    \n",
    "myStreamingAL2, myStepSizes2 = myActiveStrategy5.run_StreamingAL(update_streaming_perc, num_steps, num_train_triplets)\n",
    "plt.plot(myStepSizes2, myStreamingAL2 , '--b', marker=\"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Margin Sampling-Based Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import operator\n",
    "\n",
    "model6 = FaceNetModel(embedding_size = embedding_size, num_classes = num_classes).to(device)\n",
    "myActiveStrategy6 = ActiveStrategy(model6, num_steps, margin, num_epochs, batch_size, num_workers)\n",
    "\n",
    "def update_margin(inferred_res, nRec):\n",
    "    #for k in range(len(inferred_res)):\n",
    "        # Margin difference in confidence between the most confident classes\n",
    "        #inferred_res[k].extend(list(zip(*sorted(enumerate(inferred_res[k][4]), key=operator.itemgetter(1))))[1][-2:])\n",
    "    \n",
    "    ranked = sorted(inferred_res, key=lambda x: float(abs(x[3] - x[2])), reverse=False)[:nRec] # margin low to high\n",
    "    selected = [rec[0] for rec in ranked]\n",
    "    \n",
    "    return selected\n",
    "\n",
    "myConfidenceAL4,length_labeled4 = myActiveStrategy6.run_ConfidenceAL(update_margin, num_steps, num_train_triplets)\n",
    "# plt.plot([nps*i for i in range(1, num_steps + 1)], mySupervised,    '--b',\n",
    "#          length_labeled, myConfidenceAL3, '--g',\n",
    "#          length_labeled4, myConfidenceAL4, '--p',\n",
    "#          marker=\"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W4QXsNPKsgCg"
   },
   "source": [
    "### Query-By-Committee Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ERjlDARsshJI"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model7 = FaceNetModel(embedding_size = embedding_size, num_classes = num_classes).to(device)\n",
    "myActiveStrategy7 = ActiveStrategy(model7, num_steps, margin, num_epochs, batch_size, num_workers)\n",
    "\n",
    "def QbCAL(nsteps, stepSize):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    unlabeled = [i for i in range(num_train_triplets)]\n",
    "    labeled   = []\n",
    "\n",
    "    # Randomly sample what is to be labeled first...\n",
    "    to_be_labeled = random.sample(unlabeled, stepSize)\n",
    "    unlabeled = [i for i in range(len(unlabeled)) if i not in to_be_labeled]\n",
    "    \n",
    "    for n in range(0, nsteps):\n",
    "        myAccuracy1 = myActiveStrategy7.run_one(to_be_labeled)\n",
    "        myResults1  = myActiveStrategy7.infer(unlabeled)\n",
    "        myAccuracy2 = myActiveStrategy7.run_one(to_be_labeled)\n",
    "        myResults2  = myActiveStrategy7.infer(unlabeled)\n",
    "        myAccuracy3 = myActiveStrategy7.run_one(to_be_labeled)\n",
    "        myResults3  = myActiveStrategy7.infer(unlabeled)\n",
    "        \n",
    "        disagreement = []\n",
    "\n",
    "        for r in range(len(unlabeled)):\n",
    "            dis = 0\n",
    "            if np.sign(float(myResults1[r][2]-myResults1[r][3])) != np.sign(float(myResults2[r][2]-myResults2[r][3])):\n",
    "                dis += 1\n",
    "            if np.sign(float(myResults1[r][2]-myResults1[r][3])) != np.sign(float(myResults3[r][2]-myResults3[r][3])):\n",
    "                dis += 1\n",
    "            if np.sign(float(myResults2[r][2]-myResults2[r][3])) != np.sign(float(myResults3[r][2]-myResults3[r][3])):   \n",
    "                dis += 1\n",
    "            disagreement.append(dis)\n",
    "        \n",
    "        sorted_by_disagree = sorted(zip(myResults1, disagreement), key=lambda x: x[-1], reverse=True)[:stepSize] # fill here; remember that the size of the array will be 'nps'\n",
    "\n",
    "        # update to_be_labeled:\n",
    "        to_be_labeled.extend([x[0] for x in [row[0] for row in sorted_by_disagree]])\n",
    "        to_be_labeled_set = set(to_be_labeled)\n",
    "        unlabeled = list(filter(lambda x: x not in to_be_labeled_set, unlabeled)) # update unlabeled\n",
    "\n",
    "        myres = myActiveStrategy7.run_one(to_be_labeled)\n",
    "        results.append(myres)        \n",
    "        \n",
    "    return results\n",
    "    \n",
    "myQbCAL = QbCAL(num_steps, nps)\n",
    "# plt.plot([nps*i for i in range(1, num_steps + 1)], mySupervised,    '--b',\n",
    "#          length_labeled4, myConfidenceAL4, '--g',\n",
    "#          [nps*i for i in range(1, num_steps + 1)], myQbCAL,         '--p',\n",
    "#          marker=\"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([nps*i for i in range(1, num_steps + 1)], mySupervised,    '--b', label=\"Baseline\",marker=\"o\")\n",
    "plt.plot([nps*i for i in range(1, num_steps + 1)], myConfidenceAL,  '--p', label=\"Uncertainty-Based Strategy\",marker=\"o\")\n",
    "plt.plot([nps*i for i in range(1, num_steps + 1)], myConfidenceAL2, '--p', label=\"ConfidenceLevel Strategy\",marker=\"o\")\n",
    "plt.plot(length_labeled, myConfidenceAL3, '--p', label=\"Better ConfidenceLevel Strategy\",marker=\"o\")\n",
    "plt.plot(myStepSizes2, myStreamingAL2 , '--p', label=\"Streaming Strategy\",marker=\"o\")\n",
    "plt.plot(length_labeled4, myConfidenceAL4, '--p', label=\"Margin Sampling-Based Strategy\",marker=\"o\")\n",
    "plt.plot([nps*i for i in range(1, num_steps + 1)], myQbCAL,'--p', label=\"Query-By-Committee Strategy\",marker=\"o\")\n",
    "plt.legend(bbox_to_anchor=(1.63, 1.03))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Facial Recognition.ipynb",
   "provenance": [
    {
     "file_id": "1-4fpY8lZJ1h9vW3WpJIDf9D2lJ8rWg3P",
     "timestamp": 1580608510738
    },
    {
     "file_id": "1erC_ve9xYZhb-8da2Wpbme3DDBkQ-Ijj",
     "timestamp": 1580447624509
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
